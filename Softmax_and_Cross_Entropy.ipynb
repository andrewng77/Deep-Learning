{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Softmax and Cross Entropy",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM3kKoXF7ZkZXRuk0zWcmmk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBOJvefs6Qf0",
        "colab_type": "text"
      },
      "source": [
        "# Softmax and Cross Entropy<p>\n",
        "\n",
        "\n",
        "Softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRd_BV2W6P4-",
        "colab_type": "code",
        "outputId": "c9402432-242a-4d00-c1eb-cbc8cc0fd158",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "  return np.exp(x)/ np.sum(np.exp(x),axis=0)\n",
        "\n",
        "x= np.array([2.0,1.0,0.1])\n",
        "outputs = softmax(x)\n",
        "print('softmax numpy:',outputs)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "softmax numpy: [0.65900114 0.24243297 0.09856589]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5hLmrkr8kdp",
        "colab_type": "code",
        "outputId": "7ed24020-a001-44a5-fe15-7bd1098211d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = torch.tensor([2.0,1.0,0.1])\n",
        "torch.softmax(x,dim=0) #along the first axis\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6590, 0.2424, 0.0986])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihe4Wpyw9b4S",
        "colab_type": "text"
      },
      "source": [
        "Cross Entropy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIKWh-F689fY",
        "colab_type": "code",
        "outputId": "d8d5f734-91d1-4e67-d542-368c1986b281",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def cross_entropy(actual,predicted):\n",
        "  loss = - np.sum(actual * np.log(predicted))\n",
        "  return loss\n",
        "\n",
        "# y must be one hot encoded\n",
        "# if class 0: [1 0 0]\n",
        "# if class 1: [0 1 0]\n",
        "# if class 2: [0 0 1]\n",
        "\n",
        "Y = np.array([1,0,0])\n",
        "\n",
        "# y_pred has probabilities\n",
        "Y_pred_good = np.array([0.7, 0.2, 0.1])\n",
        "Y_pred_bad = np.array([0.1, 0.3, 0.6])\n",
        "\n",
        "l1 = cross_entropy(Y,Y_pred_good)\n",
        "l2 = cross_entropy(Y,Y_pred_bad)\n",
        "\n",
        "print(f'Loss1 numpy : {l1:.4f}') #lower \n",
        "print(f'Loss2 numpy : {l2:.4f}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss1 numpy : 0.3567\n",
            "Loss2 numpy : 2.3026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3cSsy-I_r6w",
        "colab_type": "code",
        "outputId": "68159c03-abbc-4135-971d-a29a1310554c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "loss = nn.CrossEntropyLoss() #Softmax at the last layer not required\n",
        "# nn.CrossEntropyLoss() = nn.LogSoftmax + nn.NLLLoss (negative log likelihood loss)\n",
        "\n",
        "# Y actual ( no One Hot required )\n",
        "# Y_pred has raw scores(logits), requires softmax \n",
        "\n",
        "Y = torch.tensor([0])\n",
        "# nsamples x nclasses = 1 x 3\n",
        "Y_pred_good = torch.tensor([2.0,1.0,0.1]).view(1,3)\n",
        "Y_pred_bad = torch.tensor([[0.5,2.0,0.1]]) #or can be written as this\n",
        "\n",
        "l1 = loss ( Y_pred_good,Y )\n",
        "l2 = loss ( Y_pred_bad, Y )\n",
        "\n",
        "print(f'Loss1 numpy : {l1.item()}') #lower \n",
        "print(f'Loss2 numpy : {l2.item()}')\n",
        "\n",
        "_, predictions1 = torch.max(Y_pred_good,1)\n",
        "_, predictions2 = torch.max(Y_pred_bad,1)\n",
        "print(predictions1)\n",
        "print(predictions2)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss1 numpy : 0.4170299470424652\n",
            "Loss2 numpy : 1.8167786598205566\n",
            "tensor([0])\n",
            "tensor([1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEkianzgN4U_",
        "colab_type": "code",
        "outputId": "46d98544-702f-4ff3-911d-1ddd2e0a08b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "loss = nn.CrossEntropyLoss() #Softmax at the last layer not required\n",
        "# nn.CrossEntropyLoss() = nn.LogSoftmax + nn.NLLLoss (negative log likelihood loss)\n",
        "\n",
        "# Y actual ( no One Hot required )\n",
        "# Y_pred has raw scores(logits), requires softmax \n",
        "\n",
        "Y = torch.tensor([2])\n",
        "# nsamples x nclasses = 1 x 3\n",
        "Y_pred_bad = torch.tensor([2.0,1.0,0.1]).view(1,3)\n",
        "Y_pred_good = torch.tensor([[0.5,2.0,2]]) #or can be written as this\n",
        "\n",
        "#l1 and l2 is swapped\n",
        "l1 = loss ( Y_pred_good,Y )\n",
        "l2 = loss ( Y_pred_bad, Y )\n",
        "\n",
        "print(f'Loss1 numpy : {l1.item()}') #lower \n",
        "print(f'Loss2 numpy : {l2.item()}') \n",
        "\n",
        "_, predictions1 = torch.max(Y_pred_good,dim=1)\n",
        "_, predictions2 = torch.max(Y_pred_bad,dim=1)\n",
        "print(predictions1)\n",
        "print(predictions2)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss1 numpy : 0.798916220664978\n",
            "Loss2 numpy : 2.3170299530029297\n",
            "tensor([2])\n",
            "tensor([0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYIUBtJSPgfR",
        "colab_type": "text"
      },
      "source": [
        "# Multiple samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTsf7o1qPf-n",
        "colab_type": "code",
        "outputId": "26caf8be-0882-4be2-8210-8780a0c7832c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "loss = nn.CrossEntropyLoss() #Softmax at the last layer not required\n",
        "# nn.CrossEntropyLoss() = nn.LogSoftmax + nn.NLLLoss (negative log likelihood loss)\n",
        "\n",
        "# Y actual ( no One Hot required )\n",
        "# Y_pred has raw scores(logits), requires softmax \n",
        "\n",
        "Y = torch.tensor([2,0,1])\n",
        "\n",
        "# nsamples x nclasses = 3 x 3\n",
        "Y_pred_good = torch.tensor(\n",
        "    [[0.1, 0.2, 3.9], # predict class 2\n",
        "    [1.2, 0.1, 0.3], # predict class 0\n",
        "    [0.3, 2.2, 0.2]]) # predict class 1\n",
        "\n",
        "Y_pred_bad = torch.tensor(\n",
        "    [[0.9, 0.2, 0.1],\n",
        "    [0.1, 0.3, 1.5],\n",
        "    [1.2, 0.2, 0.5]])\n",
        "\n",
        "l1 = loss ( Y_pred_good,Y )\n",
        "l2 = loss ( Y_pred_bad, Y )\n",
        "\n",
        "print(f'Loss1 numpy : {l1.item()}') #lower \n",
        "print(f'Loss2 numpy : {l2.item()}')\n",
        "\n",
        "_, predictions1 = torch.max(Y_pred_good,dim=1)\n",
        "_, predictions2 = torch.max(Y_pred_bad,dim=1)\n",
        "\n",
        "print(predictions1)\n",
        "print(predictions2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss1 numpy : 0.28342217206954956\n",
            "Loss2 numpy : 1.6418448686599731\n",
            "tensor([2, 0, 1])\n",
            "tensor([0, 2, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TeP_KHEYXeO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}