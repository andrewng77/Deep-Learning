{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple Gradient with Autograd ",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOc3jhHcwx0IlbWe59ERsFO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViFCvo33Jc8O",
        "colab_type": "text"
      },
      "source": [
        "# Gradient Descent\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D85KCe_pITYT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4cbf05aa-ae78-4486-b06a-9f743fa521fa"
      },
      "source": [
        "import torch\n",
        "\n",
        "# we invoke requires_grad=True when we want to find this weight\n",
        "x = torch.randn(3,requires_grad=True)\n",
        "y = x + 2\n",
        "z = y*y*2 #<-- not scalar\n",
        "#z = z.mean()\n",
        "z"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0473, 0.0029, 0.2275], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNkjg1PTKas7",
        "colab_type": "code",
        "outputId": "8055e3f1-c896-4f35-8862-2d9548e30b60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# if z is not scaler, we need to create a v vector that is the same size as z\n",
        "# https://medium.com/unit8-machine-learning-publication/computing-the-jacobian-matrix-of-a-neural-network-in-python-4f162e5db180\n",
        "v = torch.randn(3)\n",
        "v"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.9347,  0.3953,  0.2299])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcyJ0_kcKI3X",
        "colab_type": "code",
        "outputId": "ec5c9f07-ed79-4213-d694-02ae358e9aeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "z.backward(v,retain_graph=True) #ok after doing this\n",
        "x.grad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-32.5362,  20.9846,   5.4597])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx6VRqzn7gzN",
        "colab_type": "text"
      },
      "source": [
        "Calculate weights without computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_hw7cy4LDuZ",
        "colab_type": "code",
        "outputId": "68b9484a-01e8-4eb8-9bd3-668058882769",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = torch.randn(3,requires_grad=True)\n",
        "x\n",
        "\n",
        "# x.requires_grad_(False)\n",
        "# x.detach()\n",
        "# with torch.no_grad():"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0512, 0.0848, 0.1704], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNVE6ubPMxI-",
        "colab_type": "code",
        "outputId": "4f9ddbf4-e800-4544-f703-6e858d295c82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# option 1\n",
        "x.requires_grad_(False)\n",
        "x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 2.3975,  1.0253, -1.4252])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdmaGbx8M7YA",
        "colab_type": "code",
        "outputId": "17982255-2f7d-4c74-f6d9-e2a8e66d0538",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# option 2\n",
        "y=x.detach()\n",
        "y"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.2284, -0.0936,  1.3454])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL-tZHChNDyN",
        "colab_type": "code",
        "outputId": "aad8789a-a76b-4aec-e50c-f98b0da479ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# option 3\n",
        "with torch.no_grad():\n",
        "  y = x + 2\n",
        "  print(y)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3.0512, 2.0848, 2.1704])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luLMvWeaN-oU",
        "colab_type": "text"
      },
      "source": [
        "Failed Example on how NOT to reset the grad ( fail to reset )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbFzGV4POBgT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "1548aa3b-772c-4427-a0a9-128646769136"
      },
      "source": [
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "  model_output = (weights *3).sum()\n",
        "  model_output.backward()\n",
        "\n",
        "  print(weights.grad) # the gradients are accumulated in every loop\n",
        "  #weights.grad.zero_() #reset zero"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([6., 6., 6., 6.])\n",
            "tensor([9., 9., 9., 9.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3a6v_OnOHHp",
        "colab_type": "text"
      },
      "source": [
        "We managed to reset the grad here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnKoYIpDNNlm",
        "colab_type": "code",
        "outputId": "ae3cf014-2e74-4c5c-de36-1f307b5aafb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "  model_output = (weights *3).sum()\n",
        "  model_output.backward()\n",
        "\n",
        "  print(weights.grad) # the gradients are NOT summed in every loop\n",
        "  weights.grad.zero_() #reset zero"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y24X5s2SZivZ",
        "colab_type": "text"
      },
      "source": [
        "Simple Backprop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heqn7zHiPFLI",
        "colab_type": "code",
        "outputId": "73b43a52-0d6a-482b-e23f-0e87c6fc4475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(1.0)\n",
        "y = torch.tensor(2.0)\n",
        "w = torch.tensor(1.0,requires_grad=True)\n",
        "\n",
        "# forward pass and compute the loss and local gradient\n",
        "y_hat = w*x\n",
        "loss = (y_hat - y)**2\n",
        "\n",
        "print(loss)\n",
        "\n",
        "# backward pass\n",
        "loss.backward()\n",
        "print(w.grad)\n",
        "\n",
        "## update weights\n",
        "## next forward backward pass\n",
        "\n",
        "w.sub(w.grad)\n",
        "w"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1., grad_fn=<PowBackward0>)\n",
            "tensor(-2.)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1., requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDIlGECj7Xqv",
        "colab_type": "text"
      },
      "source": [
        "Gradient Descent from Scratch using numpy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvra7DAG7W9F",
        "colab_type": "code",
        "outputId": "ab807d43-5b0a-4b84-c449-42a006e8479d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "import torch\n",
        "np.random.seed(42)\n",
        "\n",
        "# f = w * x\n",
        "# f = 2 * x\n",
        "\n",
        "X = np.array([1,2,3,4],dtype = np.float32)\n",
        "Y = np.array([2,4,6,8],dtype = np.float32)\n",
        "\n",
        "#initialize the weight\n",
        "#w = np.random.rand()\n",
        "w = 0.0\n",
        "print(w)\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_hat):\n",
        "  return ((y_hat - y)**2).mean()\n",
        "\n",
        "# gradient\n",
        "# MSE = 1/N * (w*x - y)**2\n",
        "# dJ/dw = 1/N 2x (w*x-y)\n",
        "\n",
        "def gradient(x,y,y_hat):\n",
        "  return np.dot(2*x,y_hat-y)/x.size\n",
        "\n",
        "print(f'Prediction before training : f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_hat = forward(X)\n",
        "  \n",
        "  # loss\n",
        "  l=loss(Y,y_hat)\n",
        "\n",
        "  # gradients\n",
        "  dw = gradient(X,Y,y_hat)\n",
        "\n",
        "  # update weights\n",
        "  w -= learning_rate * dw\n",
        "\n",
        "  if epoch % 10==0:\n",
        "    print(f'epoch: {epoch+1}, weight : {w:.3f}, loss ={l:.8f}')\n",
        "\n",
        "print(f'Prediction after training : f(5) = {forward(5):.3f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0\n",
            "Prediction before training : f(5) = 0.000\n",
            "epoch: 1, weight : 0.300, loss =30.00000000\n",
            "epoch: 11, weight : 1.665, loss =1.16278565\n",
            "epoch: 21, weight : 1.934, loss =0.04506905\n",
            "epoch: 31, weight : 1.987, loss =0.00174685\n",
            "epoch: 41, weight : 1.997, loss =0.00006770\n",
            "epoch: 51, weight : 1.999, loss =0.00000262\n",
            "epoch: 61, weight : 2.000, loss =0.00000010\n",
            "epoch: 71, weight : 2.000, loss =0.00000000\n",
            "epoch: 81, weight : 2.000, loss =0.00000000\n",
            "epoch: 91, weight : 2.000, loss =0.00000000\n",
            "Prediction after training : f(5) = 10.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsPfFUh0D-cJ",
        "colab_type": "text"
      },
      "source": [
        "Gradient Descent - Autograd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYai-3rSo4VT",
        "colab_type": "text"
      },
      "source": [
        "PyTorch Tutorial 06 - Training Pipeline: Model, Loss, and Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOXi9NWypGxO",
        "colab_type": "text"
      },
      "source": [
        "## Step by step <p>\n",
        "1) Design model ( input, output size, forward pass )<p>\n",
        "2) Construct loss and optimizer<p>\n",
        "3) Training loop<p>\n",
        "\n",
        "- forward pass : compute prediction\n",
        "- backward pass : gradients\n",
        "- update weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3Jiz5Gd9PUy",
        "colab_type": "code",
        "outputId": "6c58d113-53a0-49bc-a7d0-785fcb30b95c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "np.random.seed(42)\n",
        "\n",
        "# f = w * x\n",
        "# f = 2 * x\n",
        "\n",
        "X = torch.tensor([1,2,3,4],dtype = torch.float32)\n",
        "Y = torch.tensor([2,4,6,8],dtype = torch.float32)\n",
        "\n",
        "#initialize the weight\n",
        "w = torch.tensor(0,dtype = torch.float32, requires_grad=True) \n",
        "print(w)\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "# loss = MSE\n",
        "def loss(y, y_hat):\n",
        "  return ((y_hat - y)**2).mean()\n",
        "\n",
        "print(f'Prediction before training : f(5) = {forward(5):.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_hat = forward(X)\n",
        "  \n",
        "  # loss\n",
        "  l=loss(Y,y_hat)\n",
        "\n",
        "  # gradients = backward pass\n",
        "  l.backward() # dl/dw\n",
        "  print(w.grad)\n",
        "\n",
        "  # update weights\n",
        "  with torch.no_grad(): #ensure the gradient is not calculated\n",
        "    w.sub_(learning_rate*w.grad)\n",
        "\n",
        "  # zero the gradients to ensure it's not accumulated\n",
        "  w.grad.zero_() #reset zero\n",
        "\n",
        "  if epoch % 10==0:\n",
        "    print(f'epoch: {epoch+1}, weight : {w:.3f}, loss ={l:.8f}')\n",
        "\n",
        "print(f'Prediction after training : f(5) = {forward(5):.3f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0., requires_grad=True)\n",
            "Prediction before training : f(5) = 0.000\n",
            "tensor(-30.)\n",
            "epoch: 1, weight : 0.300, loss =30.00000000\n",
            "tensor(-25.5000)\n",
            "tensor(-21.6750)\n",
            "tensor(-18.4238)\n",
            "tensor(-15.6602)\n",
            "tensor(-13.3112)\n",
            "tensor(-11.3145)\n",
            "tensor(-9.6173)\n",
            "tensor(-8.1747)\n",
            "tensor(-6.9485)\n",
            "tensor(-5.9062)\n",
            "epoch: 11, weight : 1.665, loss =1.16278565\n",
            "tensor(-5.0203)\n",
            "tensor(-4.2673)\n",
            "tensor(-3.6272)\n",
            "tensor(-3.0831)\n",
            "tensor(-2.6206)\n",
            "tensor(-2.2275)\n",
            "tensor(-1.8934)\n",
            "tensor(-1.6094)\n",
            "tensor(-1.3680)\n",
            "tensor(-1.1628)\n",
            "epoch: 21, weight : 1.934, loss =0.04506890\n",
            "tensor(-0.9884)\n",
            "tensor(-0.8401)\n",
            "tensor(-0.7141)\n",
            "tensor(-0.6070)\n",
            "tensor(-0.5159)\n",
            "tensor(-0.4385)\n",
            "tensor(-0.3728)\n",
            "tensor(-0.3168)\n",
            "tensor(-0.2693)\n",
            "tensor(-0.2289)\n",
            "epoch: 31, weight : 1.987, loss =0.00174685\n",
            "tensor(-0.1946)\n",
            "tensor(-0.1654)\n",
            "tensor(-0.1406)\n",
            "tensor(-0.1195)\n",
            "tensor(-0.1016)\n",
            "tensor(-0.0863)\n",
            "tensor(-0.0734)\n",
            "tensor(-0.0624)\n",
            "tensor(-0.0530)\n",
            "tensor(-0.0451)\n",
            "epoch: 41, weight : 1.997, loss =0.00006770\n",
            "tensor(-0.0383)\n",
            "tensor(-0.0326)\n",
            "tensor(-0.0277)\n",
            "tensor(-0.0235)\n",
            "tensor(-0.0200)\n",
            "tensor(-0.0170)\n",
            "tensor(-0.0144)\n",
            "tensor(-0.0123)\n",
            "tensor(-0.0104)\n",
            "tensor(-0.0089)\n",
            "epoch: 51, weight : 1.999, loss =0.00000262\n",
            "tensor(-0.0075)\n",
            "tensor(-0.0064)\n",
            "tensor(-0.0054)\n",
            "tensor(-0.0046)\n",
            "tensor(-0.0039)\n",
            "tensor(-0.0033)\n",
            "tensor(-0.0028)\n",
            "tensor(-0.0024)\n",
            "tensor(-0.0021)\n",
            "tensor(-0.0017)\n",
            "epoch: 61, weight : 2.000, loss =0.00000010\n",
            "tensor(-0.0015)\n",
            "tensor(-0.0013)\n",
            "tensor(-0.0011)\n",
            "tensor(-0.0009)\n",
            "tensor(-0.0008)\n",
            "tensor(-0.0007)\n",
            "tensor(-0.0006)\n",
            "tensor(-0.0005)\n",
            "tensor(-0.0004)\n",
            "tensor(-0.0003)\n",
            "epoch: 71, weight : 2.000, loss =0.00000000\n",
            "tensor(-0.0003)\n",
            "tensor(-0.0002)\n",
            "tensor(-0.0002)\n",
            "tensor(-0.0002)\n",
            "tensor(-0.0002)\n",
            "tensor(-0.0001)\n",
            "tensor(-0.0001)\n",
            "tensor(-9.2983e-05)\n",
            "tensor(-7.8678e-05)\n",
            "tensor(-6.6340e-05)\n",
            "epoch: 81, weight : 2.000, loss =0.00000000\n",
            "tensor(-5.5254e-05)\n",
            "tensor(-4.6849e-05)\n",
            "tensor(-3.8981e-05)\n",
            "tensor(-3.3796e-05)\n",
            "tensor(-2.8610e-05)\n",
            "tensor(-2.4676e-05)\n",
            "tensor(-2.1458e-05)\n",
            "tensor(-1.8239e-05)\n",
            "tensor(-1.4305e-05)\n",
            "tensor(-1.2338e-05)\n",
            "epoch: 91, weight : 2.000, loss =0.00000000\n",
            "tensor(-1.0371e-05)\n",
            "tensor(-9.1195e-06)\n",
            "tensor(-7.1526e-06)\n",
            "tensor(-5.1856e-06)\n",
            "tensor(-5.1856e-06)\n",
            "tensor(-5.1856e-06)\n",
            "tensor(-5.1856e-06)\n",
            "tensor(-5.1856e-06)\n",
            "tensor(-5.1856e-06)\n",
            "Prediction after training : f(5) = 10.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjhY6JHvB0S8",
        "colab_type": "code",
        "outputId": "bc825da2-7f21-4535-a322-6985960c99b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn \n",
        "\n",
        "# f = w * x\n",
        "# f = 2 * x\n",
        "\n",
        "# 0) Training samples, watch the shape!\n",
        "# bias are taken care of\n",
        "X = torch.tensor([[1],[2],[3],[4]],dtype = torch.float32)\n",
        "Y = torch.tensor([[2],[4],[6],[8]],dtype = torch.float32)\n",
        "\n",
        "# test sample\n",
        "X_test = torch.tensor([5],dtype = torch.float32)\n",
        "\n",
        "n_samples,n_features = X.shape\n",
        "print(n_samples, n_features)\n",
        "\n",
        "# 1) Design Model, the model has to implement the forward pass!\n",
        "# Here we can use a built-in model from PyTorch\n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "model = nn.Linear(input_size,output_size)\n",
        "\n",
        "print(f'Prediction before training : f(5) = {model(X_test).item():.3f}')\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "# Loss\n",
        "loss = nn.MSELoss() \n",
        "\n",
        "# optimzie the weights\n",
        "# bias all included\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_hat = model(X)\n",
        "  \n",
        "  # loss\n",
        "  l=loss(Y,y_hat)\n",
        "\n",
        "  # gradients = backward pass\n",
        "  l.backward() # dl/dw\n",
        "  print(w.grad)\n",
        "\n",
        "  # update weights\n",
        "  optimizer.step()\n",
        "  \n",
        "  # zero gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # zero the gradients to ensure it's not accumulated\n",
        "  w.grad.zero_() #reset zero\n",
        "\n",
        "  if epoch % 10==0:\n",
        "    [w,b] = model.parameters()\n",
        "    print(f'epoch: {epoch+1}, weight : {w[0][0].item():.3f}, loss ={l:.8f}')\n",
        "\n",
        "print(f'Prediction after training : f(5) = {model(X_test).item():.3f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4 1\n",
            "Prediction before training : f(5) = -2.434\n",
            "tensor(0.)\n",
            "epoch: 1, weight : 0.020, loss =49.51508331\n",
            "tensor([[-32.0879]])\n",
            "tensor([[-26.7319]])\n",
            "tensor([[-22.2705]])\n",
            "tensor([[-18.5541]])\n",
            "tensor([[-15.4583]])\n",
            "tensor([[-12.8796]])\n",
            "tensor([[-10.7315]])\n",
            "tensor([[-8.9422]])\n",
            "tensor([[-7.4516]])\n",
            "tensor([[-6.2100]])\n",
            "epoch: 11, weight : 1.633, loss =1.28582001\n",
            "tensor([[-5.1758]])\n",
            "tensor([[-4.3142]])\n",
            "tensor([[-3.5966]])\n",
            "tensor([[-2.9988]])\n",
            "tensor([[-2.5008]])\n",
            "tensor([[-2.0859]])\n",
            "tensor([[-1.7404]])\n",
            "tensor([[-1.4525]])\n",
            "tensor([[-1.2127]])\n",
            "tensor([[-1.0130]])\n",
            "epoch: 21, weight : 1.894, loss =0.03773768\n",
            "tensor([[-0.8466]])\n",
            "tensor([[-0.7080]])\n",
            "tensor([[-0.5925]])\n",
            "tensor([[-0.4963]])\n",
            "tensor([[-0.4161]])\n",
            "tensor([[-0.3494]])\n",
            "tensor([[-0.2937]])\n",
            "tensor([[-0.2474]])\n",
            "tensor([[-0.2088]])\n",
            "tensor([[-0.1766]])\n",
            "epoch: 31, weight : 1.937, loss =0.00518674\n",
            "tensor([[-0.1498]])\n",
            "tensor([[-0.1275]])\n",
            "tensor([[-0.1089]])\n",
            "tensor([[-0.0933]])\n",
            "tensor([[-0.0804]])\n",
            "tensor([[-0.0696]])\n",
            "tensor([[-0.0606]])\n",
            "tensor([[-0.0531]])\n",
            "tensor([[-0.0469]])\n",
            "tensor([[-0.0417]])\n",
            "epoch: 41, weight : 1.946, loss =0.00409949\n",
            "tensor([[-0.0373]])\n",
            "tensor([[-0.0337]])\n",
            "tensor([[-0.0307]])\n",
            "tensor([[-0.0281]])\n",
            "tensor([[-0.0260]])\n",
            "tensor([[-0.0242]])\n",
            "tensor([[-0.0228]])\n",
            "tensor([[-0.0215]])\n",
            "tensor([[-0.0205]])\n",
            "tensor([[-0.0196]])\n",
            "epoch: 51, weight : 1.948, loss =0.00384057\n",
            "tensor([[-0.0188]])\n",
            "tensor([[-0.0182]])\n",
            "tensor([[-0.0177]])\n",
            "tensor([[-0.0173]])\n",
            "tensor([[-0.0169]])\n",
            "tensor([[-0.0166]])\n",
            "tensor([[-0.0163]])\n",
            "tensor([[-0.0160]])\n",
            "tensor([[-0.0158]])\n",
            "tensor([[-0.0156]])\n",
            "epoch: 61, weight : 1.950, loss =0.00361649\n",
            "tensor([[-0.0155]])\n",
            "tensor([[-0.0154]])\n",
            "tensor([[-0.0152]])\n",
            "tensor([[-0.0151]])\n",
            "tensor([[-0.0150]])\n",
            "tensor([[-0.0149]])\n",
            "tensor([[-0.0149]])\n",
            "tensor([[-0.0148]])\n",
            "tensor([[-0.0147]])\n",
            "tensor([[-0.0146]])\n",
            "epoch: 71, weight : 1.952, loss =0.00340598\n",
            "tensor([[-0.0146]])\n",
            "tensor([[-0.0145]])\n",
            "tensor([[-0.0145]])\n",
            "tensor([[-0.0144]])\n",
            "tensor([[-0.0144]])\n",
            "tensor([[-0.0143]])\n",
            "tensor([[-0.0143]])\n",
            "tensor([[-0.0142]])\n",
            "tensor([[-0.0142]])\n",
            "tensor([[-0.0141]])\n",
            "epoch: 81, weight : 1.953, loss =0.00320775\n",
            "tensor([[-0.0141]])\n",
            "tensor([[-0.0140]])\n",
            "tensor([[-0.0140]])\n",
            "tensor([[-0.0140]])\n",
            "tensor([[-0.0139]])\n",
            "tensor([[-0.0139]])\n",
            "tensor([[-0.0138]])\n",
            "tensor([[-0.0138]])\n",
            "tensor([[-0.0137]])\n",
            "tensor([[-0.0137]])\n",
            "epoch: 91, weight : 1.954, loss =0.00302103\n",
            "tensor([[-0.0137]])\n",
            "tensor([[-0.0136]])\n",
            "tensor([[-0.0136]])\n",
            "tensor([[-0.0135]])\n",
            "tensor([[-0.0135]])\n",
            "tensor([[-0.0135]])\n",
            "tensor([[-0.0134]])\n",
            "tensor([[-0.0134]])\n",
            "tensor([[-0.0133]])\n",
            "Prediction after training : f(5) = 9.909\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVc-QVxmkPEd",
        "colab_type": "code",
        "outputId": "15571091-dbc2-4700-b7d9-eef58ca2b640",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "#1) Design model ( input, output size, forward pass )\n",
        "#2) Construct loss and optimizer\n",
        "#3) Training loop\n",
        "#forward pass : compute prediction\n",
        "#backward pass : gradients\n",
        "#update weights\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#np.random.seed(42)\n",
        "\n",
        "# f = w * x\n",
        "# f = 2 * x\n",
        "\n",
        "# 0) Training samples, watch the shape!\n",
        "# bias are taken care of\n",
        "X = torch.tensor([[1],[2],[3],[4]],dtype = torch.float32)\n",
        "Y = torch.tensor([[2],[4],[6],[8]],dtype = torch.float32)\n",
        "\n",
        "X_test = torch.tensor([[5]],dtype = torch.float32)\n",
        "n_samples,n_features = X.shape\n",
        "print(X.shape)\n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "model = nn.Linear(input_size,output_size)\n",
        "\n",
        "#initialize the weight\n",
        "w = torch.tensor(0.0,dtype = torch.float32, requires_grad=True) \n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "# loss = MSE\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "\n",
        "print(f'Prediction before training : f(5) = {model(X_test).item():.3f}')\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_hat = model(X)\n",
        "  \n",
        "  # loss\n",
        "  l=loss(Y,y_hat)\n",
        "\n",
        "  # gradients = backward pass\n",
        "  l.backward() # dl/dw\n",
        "\n",
        "  # update weights\n",
        "  #with torch.no_grad(): #ensure the gradient is not calculated\n",
        "  optimizer.step()\n",
        "\n",
        "  # zero the gradients to ensure it's not accumulated\n",
        "  optimizer.zero_grad() #reset zero\n",
        "\n",
        "  if epoch % 10==0:\n",
        "    w,b = model.parameters()\n",
        "    print(f'epoch: {epoch+1}, weight : {w.item():.3f}, loss ={l.item():.8f}')\n",
        "\n",
        "print(f'Prediction after training : f(5) = {model(X_test).item():.3f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 1])\n",
            "Prediction before training : f(5) = -1.191\n",
            "epoch: 1, weight : 0.030, loss =36.22446060\n",
            "epoch: 11, weight : 1.415, loss =1.08311355\n",
            "epoch: 21, weight : 1.646, loss =0.16543093\n",
            "epoch: 31, weight : 1.690, loss =0.13369039\n",
            "epoch: 41, weight : 1.705, loss =0.12533686\n",
            "epoch: 51, weight : 1.715, loss =0.11802671\n",
            "epoch: 61, weight : 1.723, loss =0.11115661\n",
            "epoch: 71, weight : 1.732, loss =0.10468666\n",
            "epoch: 81, weight : 1.739, loss =0.09859339\n",
            "epoch: 91, weight : 1.747, loss =0.09285477\n",
            "Prediction after training : f(5) = 9.493\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lojngyRssEnk",
        "colab_type": "code",
        "outputId": "06445bf4-6d15-466c-8e7f-c55e73ddebf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "#1) Design model ( input, output size, forward pass )\n",
        "#2) Construct loss and optimizer\n",
        "#3) Training loop\n",
        "#forward pass : compute prediction\n",
        "#backward pass : gradients\n",
        "#update weights\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#np.random.seed(42)\n",
        "\n",
        "# f = w * x\n",
        "# f = 2 * x\n",
        "\n",
        "# 0) Training samples, watch the shape!\n",
        "# bias are taken care of\n",
        "X = torch.tensor([[1],[2],[3],[4]],dtype = torch.float32)\n",
        "Y = torch.tensor([[2],[4],[6],[8]],dtype = torch.float32)\n",
        "\n",
        "X_test = torch.tensor([[5]],dtype = torch.float32)\n",
        "n_samples,n_features = X.shape\n",
        "print(X.shape)\n",
        "\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "#model = nn.Linear(input_size,output_size)\n",
        "\n",
        "#initialize the weight\n",
        "w = torch.tensor(0.0,dtype = torch.float32, requires_grad=True) \n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "class LinearRegression(nn.Module):\n",
        "  \n",
        "  def __init__(self,input_dim,output_dim):\n",
        "    super(LinearRegression,self).__init__()\n",
        "    # define layers\n",
        "    self.lin = nn.Linear(input_dim,output_dim)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    return self.lin(x)\n",
        "\n",
        "model = LinearRegression(input_size,output_size)\n",
        "\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "# loss = MSE\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "\n",
        "print(f'Prediction before training : f(5) = {model(X_test).item():.3f}')\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # prediction = forward pass\n",
        "  y_hat = model(X)\n",
        "  \n",
        "  # loss\n",
        "  l=loss(Y,y_hat)\n",
        "\n",
        "  # gradients = backward pass\n",
        "  l.backward() # dl/dw\n",
        "\n",
        "  # update weights\n",
        "  #with torch.no_grad(): #ensure the gradient is not calculated\n",
        "  optimizer.step()\n",
        "\n",
        "  # zero the gradients to ensure it's not accumulated\n",
        "  optimizer.zero_grad() #reset zero\n",
        "\n",
        "  if epoch % 10==0:\n",
        "    w,b = model.parameters()\n",
        "    print(f'epoch: {epoch+1}, weight : {w.item():.3f}, loss ={l.item():.8f}')\n",
        "\n",
        "print(f'Prediction after training : f(5) = {model(X_test).item():.3f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 1])\n",
            "Prediction before training : f(5) = 3.698\n",
            "epoch: 1, weight : 1.046, loss =13.34873009\n",
            "epoch: 11, weight : 1.881, loss =0.34783754\n",
            "epoch: 21, weight : 2.015, loss =0.01132941\n",
            "epoch: 31, weight : 2.035, loss =0.00248756\n",
            "epoch: 41, weight : 2.038, loss =0.00213108\n",
            "epoch: 51, weight : 2.037, loss =0.00200157\n",
            "epoch: 61, weight : 2.036, loss =0.00188492\n",
            "epoch: 71, weight : 2.035, loss =0.00177521\n",
            "epoch: 81, weight : 2.034, loss =0.00167188\n",
            "epoch: 91, weight : 2.033, loss =0.00157456\n",
            "Prediction after training : f(5) = 10.066\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHuQQfqo9uBc",
        "colab_type": "text"
      },
      "source": [
        "# Linear  Regression\n",
        "\n",
        "### Step by step <p>\n",
        "1) Design model ( input, output size, forward pass )<p>\n",
        "2) Construct loss and optimizer<p>\n",
        "3) Training loop<p>\n",
        "\n",
        "- forward pass : compute prediction\n",
        "- backward pass : gradients\n",
        "- update weights\n",
        "\n",
        "### General Steps <p>\n",
        "0) prepare data <p>\n",
        "1) Model <p>\n",
        "2) loss and optimizer <p>\n",
        "3) training loop <p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoFO9dV09tEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUVk6q9k-boU",
        "colab_type": "code",
        "outputId": "97d34532-c8c2-4542-bd45-475549109f3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 0) prepare data\n",
        "bc = datasets.load_breast_cancer()\n",
        "X,y = bc.data, bc.target\n",
        "\n",
        "set(y) #binary response ( logistic regression)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "print(n_samples,n_features)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# scale ( normalize to mean = 0 , s.d = 1)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "#convert numpy to tensor\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "#reshape to (len(tensor),1)\n",
        "y_train = y_train.view(y_train.shape[0],1)\n",
        "y_test = y_test.view(y_test.shape[0],1)\n",
        "\n",
        "# 1) Model\n",
        "# f = wx + b,sigmoid in the end since it's binary\n",
        "\n",
        "class LogisticRegression(nn.Module):\n",
        "  \n",
        "  def __init__(self,n_input_features):\n",
        "    super(LogisticRegression,self).__init__()\n",
        "    # define layers\n",
        "    self.linear = nn.Linear(n_input_features,1) #output is only 1\n",
        "\n",
        "  def forward(self,x): # x is input data\n",
        "    y_predicted = torch.sigmoid(self.linear(x))\n",
        "    return y_predicted \n",
        "\n",
        "model = LogisticRegression(n_features) \n",
        "\n",
        "# 2) loss and optimizer\n",
        "# Training\n",
        "learning_rate = 0.01\n",
        "\n",
        "criterion = nn.BCELoss() #binary loss\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "\n",
        "# 3) training loop\n",
        "num_epochs = 500\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "  # prediction = forward pass\n",
        "  y_predicted = model(X_train)\n",
        "  \n",
        "  # loss\n",
        "  loss=criterion(y_predicted,y_train)\n",
        "\n",
        "  # gradients = backward pass\n",
        "  loss.backward() # dl/dw\n",
        "\n",
        "  # update weights\n",
        "  optimizer.step()\n",
        "\n",
        "  # zero the gradients to ensure it's not accumulated\n",
        "  optimizer.zero_grad() #reset zero\n",
        "\n",
        "  if (epoch+1) % 20==0:\n",
        "    w,b = model.parameters()\n",
        "    print(f'epoch: {epoch+1}, loss ={l.item():.4f}')\n",
        "\n",
        "  with torch.no_grad(): #ensure the gradient is not calculated\n",
        "    y_predicted = model(X_test) # will return a probability(sigmoid)\n",
        "    y_predicted_cls = y_predicted.round()\n",
        "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
        "    print(f'accuracy = {acc:.4f}')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "569 30\n",
            "accuracy = 0.0965\n",
            "accuracy = 0.1140\n",
            "accuracy = 0.1491\n",
            "accuracy = 0.1842\n",
            "accuracy = 0.2018\n",
            "accuracy = 0.2018\n",
            "accuracy = 0.2895\n",
            "accuracy = 0.3333\n",
            "accuracy = 0.3509\n",
            "accuracy = 0.4035\n",
            "accuracy = 0.4649\n",
            "accuracy = 0.5263\n",
            "accuracy = 0.5351\n",
            "accuracy = 0.5965\n",
            "accuracy = 0.6140\n",
            "accuracy = 0.6579\n",
            "accuracy = 0.6930\n",
            "accuracy = 0.7018\n",
            "accuracy = 0.7456\n",
            "epoch: 20, loss =0.0015\n",
            "accuracy = 0.7632\n",
            "accuracy = 0.7807\n",
            "accuracy = 0.7807\n",
            "accuracy = 0.7807\n",
            "accuracy = 0.7807\n",
            "accuracy = 0.7982\n",
            "accuracy = 0.8070\n",
            "accuracy = 0.8070\n",
            "accuracy = 0.8070\n",
            "accuracy = 0.8158\n",
            "accuracy = 0.8333\n",
            "accuracy = 0.8333\n",
            "accuracy = 0.8333\n",
            "accuracy = 0.8333\n",
            "accuracy = 0.8333\n",
            "accuracy = 0.8333\n",
            "accuracy = 0.8333\n",
            "accuracy = 0.8333\n",
            "accuracy = 0.8333\n",
            "accuracy = 0.8333\n",
            "epoch: 40, loss =0.0015\n",
            "accuracy = 0.8333\n",
            "accuracy = 0.8333\n",
            "accuracy = 0.8421\n",
            "accuracy = 0.8509\n",
            "accuracy = 0.8596\n",
            "accuracy = 0.8596\n",
            "accuracy = 0.8596\n",
            "accuracy = 0.8596\n",
            "accuracy = 0.8596\n",
            "accuracy = 0.8684\n",
            "accuracy = 0.8684\n",
            "accuracy = 0.8684\n",
            "accuracy = 0.8684\n",
            "accuracy = 0.8684\n",
            "accuracy = 0.8684\n",
            "accuracy = 0.8684\n",
            "accuracy = 0.8684\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "epoch: 60, loss =0.0015\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8860\n",
            "accuracy = 0.8860\n",
            "epoch: 80, loss =0.0015\n",
            "accuracy = 0.8860\n",
            "accuracy = 0.8860\n",
            "accuracy = 0.8860\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8772\n",
            "accuracy = 0.8860\n",
            "accuracy = 0.8860\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "epoch: 100, loss =0.0015\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "epoch: 120, loss =0.0015\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "epoch: 140, loss =0.0015\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "epoch: 160, loss =0.0015\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "epoch: 180, loss =0.0015\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.8947\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "epoch: 200, loss =0.0015\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "epoch: 220, loss =0.0015\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "epoch: 240, loss =0.0015\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "epoch: 260, loss =0.0015\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9035\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "epoch: 280, loss =0.0015\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "epoch: 300, loss =0.0015\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "epoch: 320, loss =0.0015\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "epoch: 340, loss =0.0015\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "epoch: 360, loss =0.0015\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "epoch: 380, loss =0.0015\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "epoch: 400, loss =0.0015\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "epoch: 420, loss =0.0015\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "epoch: 440, loss =0.0015\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "epoch: 460, loss =0.0015\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "epoch: 480, loss =0.0015\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "accuracy = 0.9123\n",
            "epoch: 500, loss =0.0015\n",
            "accuracy = 0.9123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwYbWSsreMR-",
        "colab_type": "text"
      },
      "source": [
        "Dataset and DataLoader - Batch Training\n",
        "\n",
        "gradient computation etc. not efficient for whole data set\n",
        " -> divide dataset into small batches\n",
        "\n",
        "'''\n",
        "training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # loop over all batches\n",
        "    for i in range(total_batches):\n",
        "        batch_x, batch_y = ...\n",
        "\n",
        "epoch = one forward and backward pass of ALL training samples\n",
        "batch_size = number of training samples used in one forward/backward pass\n",
        "number of iterations = number of passes, each pass (forward+backward) using [batch_size] number of sampes\n",
        "e.g : 100 samples, batch_size=20 -> 100/20=5 iterations for 1 epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbKFYHVTHyOv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dynwq_MVfasy",
        "colab_type": "code",
        "outputId": "b378171b-b709-42ab-d8ee-845af2cd8032",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "class WineDataset(Dataset):\n",
        "\n",
        "  def __init__(self):\n",
        "    # data loading\n",
        "    file = 'https://raw.githubusercontent.com/python-engineer/pytorchTutorial/master/data/wine/wine.csv'\n",
        "    xy = np.loadtxt(file,delimiter =\",\",dtype=np.float32,skiprows=1)\n",
        "    self.x = torch.from_numpy(xy[:,1:])\n",
        "    self.y = torch.from_numpy(xy[:,[0]])\n",
        "    self.n_samples = xy.shape[0]\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.x[index],self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    # len(dataset)\n",
        "    return self.n_samples\n",
        "\n",
        "dataset = WineDataset()\n",
        "#first_data = dataset[0]\n",
        "#features,labels = first_data\n",
        "batch_size = 4\n",
        "dataloader = DataLoader(dataset=dataset, batch_size=batch_size,shuffle=True,num_workers = 2)\n",
        "dataiter = iter(dataloader)\n",
        "data = dataiter.next()\n",
        "features,labels = data\n",
        "print(features,labels)\n",
        "\n",
        "# training loop\n",
        "num_epochs = 2\n",
        "total_samples = len(dataset)\n",
        "learning_rate = 0.01\n",
        "n_iterations = math.ceil(total_samples/batch_size)\n",
        "print(total_samples,n_iterations)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for iter,(inputs,labels) in enumerate(dataloader):\n",
        "    # forward, backward, update\n",
        "    if (iter+1) % 5==0:\n",
        "      print(f'epoch :{epoch+1}/{num_epochs},step :{iter+1}/{len(dataloader)},inputs :{inputs.shape}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.3050e+01, 1.7300e+00, 2.0400e+00, 1.2400e+01, 9.2000e+01, 2.7200e+00,\n",
            "         3.2700e+00, 1.7000e-01, 2.9100e+00, 7.2000e+00, 1.1200e+00, 2.9100e+00,\n",
            "         1.1500e+03],\n",
            "        [1.3580e+01, 2.5800e+00, 2.6900e+00, 2.4500e+01, 1.0500e+02, 1.5500e+00,\n",
            "         8.4000e-01, 3.9000e-01, 1.5400e+00, 8.6600e+00, 7.4000e-01, 1.8000e+00,\n",
            "         7.5000e+02],\n",
            "        [1.1960e+01, 1.0900e+00, 2.3000e+00, 2.1000e+01, 1.0100e+02, 3.3800e+00,\n",
            "         2.1400e+00, 1.3000e-01, 1.6500e+00, 3.2100e+00, 9.9000e-01, 3.1300e+00,\n",
            "         8.8600e+02],\n",
            "        [1.1840e+01, 2.8900e+00, 2.2300e+00, 1.8000e+01, 1.1200e+02, 1.7200e+00,\n",
            "         1.3200e+00, 4.3000e-01, 9.5000e-01, 2.6500e+00, 9.6000e-01, 2.5200e+00,\n",
            "         5.0000e+02]]) tensor([[1.],\n",
            "        [3.],\n",
            "        [2.],\n",
            "        [2.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRvSDr2x5M5f",
        "colab_type": "text"
      },
      "source": [
        "import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWXNLY2S6rl9",
        "colab_type": "code",
        "outputId": "61eb1da0-b126-459e-daa8-f903d6b8f712",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class WineDataset(Dataset):\n",
        "\n",
        "  def __init__(self,transform = None): #transform is optional\n",
        "    # data loading\n",
        "    file = 'https://raw.githubusercontent.com/python-engineer/pytorchTutorial/master/data/wine/wine.csv'\n",
        "    xy = np.loadtxt(file,delimiter =\",\",dtype=np.float32,skiprows=1)\n",
        "\n",
        "\n",
        "    self.x = xy[:,1:]\n",
        "    self.y = xy[:,[0]]\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    sample =  self.x[index],self.y[index]\n",
        "\n",
        "    if self.transform: #\n",
        "      sample=self.transform(sample)\n",
        "\n",
        "    return sample\n",
        "\n",
        "  def __len__(self):\n",
        "    # len(dataset)\n",
        "    return self.n_samples\n",
        "\n",
        "class ToTensor:\n",
        "  def __call__(self,sample):\n",
        "    inputs, targets = sample\n",
        "    return torch.from_numpy(inputs),torch.from_numpy(targets)\n",
        "\n",
        "class MulTransform:\n",
        "  def __init__(self,factor):\n",
        "    self.factor = factor\n",
        "  \n",
        "  def __call__(self,sample):\n",
        "    inputs,target = sample\n",
        "    inputs *=self.factor\n",
        "    return inputs,target\n",
        "\n",
        "print('Without Transform')\n",
        "dataset = WineDataset()\n",
        "first_data = dataset[0]\n",
        "features, labels = first_data\n",
        "print(type(features), type(labels))\n",
        "print(features, labels)\n",
        "\n",
        "print('\\nWith Tensor Transform')\n",
        "dataset = WineDataset(transform = ToTensor())\n",
        "first_data = dataset[0]\n",
        "features,labels = first_data\n",
        "print(type(features),type(labels))\n",
        "print(features, labels)\n",
        "\n",
        "print('\\nWith Tensor and Multiplication Transform')\n",
        "composed = torchvision.transforms.Compose([ToTensor(),MulTransform(2)])\n",
        "dataset = WineDataset(transform=composed)\n",
        "first_data = dataset[0]\n",
        "features,labels = first_data\n",
        "print(type(features),type(labels))\n",
        "print(features, labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Without Transform\n",
            "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
            " 2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03] [1.]\n",
            "\n",
            "With Tensor Transform\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
            "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
            "        1.0650e+03]) tensor([1.])\n",
            "\n",
            "With Tensor and Multiplication Transform\n",
            "<class 'torch.Tensor'> <class 'torch.Tensor'>\n",
            "tensor([2.8460e+01, 3.4200e+00, 4.8600e+00, 3.1200e+01, 2.5400e+02, 5.6000e+00,\n",
            "        6.1200e+00, 5.6000e-01, 4.5800e+00, 1.1280e+01, 2.0800e+00, 7.8400e+00,\n",
            "        2.1300e+03]) tensor([1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBOJvefs6Qf0",
        "colab_type": "text"
      },
      "source": [
        "PyTorch Tutorial 11 - Softmax and Cross Entropy<p>\n",
        "\n",
        "\n",
        "Softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRd_BV2W6P4-",
        "colab_type": "code",
        "outputId": "c99ab67a-f9a0-4164-9f26-2f4dea4a5048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "  return np.exp(x)/ np.sum(np.exp(x),axis=0)\n",
        "\n",
        "x= np.array([2.0,1.0,0.1])\n",
        "outputs = softmax(x)\n",
        "print('softmax numpy:',outputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "softmax numpy: [0.65900114 0.24243297 0.09856589]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5hLmrkr8kdp",
        "colab_type": "code",
        "outputId": "1bb7333a-1e78-46f0-f25a-2a390a25c819",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = torch.tensor([2.0,1.0,0.1])\n",
        "torch.softmax(x,dim=0) #along the first axis\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.6590, 0.2424, 0.0986])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihe4Wpyw9b4S",
        "colab_type": "text"
      },
      "source": [
        "Cross Entropy\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIKWh-F689fY",
        "colab_type": "code",
        "outputId": "aceae0a2-8904-4191-a54d-22da3326071d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def cross_entropy(actual,predicted):\n",
        "  loss = - np.sum(actual * np.log(predicted))\n",
        "  return loss\n",
        "\n",
        "# y must be one hot encoded\n",
        "# if class 0: [1 0 0]\n",
        "# if class 1: [0 1 0]\n",
        "# if class 2: [0 0 1]\n",
        "\n",
        "Y = np.array([1,0,0])\n",
        "\n",
        "# y_pred has probabilities\n",
        "Y_pred_good = np.array([0.7, 0.2, 0.1])\n",
        "Y_pred_bad = np.array([0.1, 0.3, 0.6])\n",
        "\n",
        "l1 = cross_entropy(Y,Y_pred_good)\n",
        "l2 = cross_entropy(Y,Y_pred_bad)\n",
        "\n",
        "print(f'Loss1 numpy : {l1:.4f}') #lower \n",
        "print(f'Loss2 numpy : {l2:.4f}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss1 numpy : 0.3567\n",
            "Loss2 numpy : 2.3026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3cSsy-I_r6w",
        "colab_type": "code",
        "outputId": "777373a0-3722-4408-a976-f7cae194d052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "loss = nn.CrossEntropyLoss() #Softmax at the last layer not required\n",
        "# nn.CrossEntropyLoss() = nn.LogSoftmax + nn.NLLLoss (negative log likelihood loss)\n",
        "\n",
        "# Y actual ( no One Hot required )\n",
        "# Y_pred has raw scores(logits), requires softmax \n",
        "\n",
        "Y = torch.tensor([0])\n",
        "# nsamples x nclasses = 1 x 3\n",
        "Y_pred_good = torch.tensor([2.0,1.0,0.1]).view(1,3)\n",
        "Y_pred_bad = torch.tensor([[0.5,2.0,0.1]]) #or can be written as this\n",
        "\n",
        "l1 = loss ( Y_pred_good,Y )\n",
        "l2 = loss ( Y_pred_bad, Y )\n",
        "\n",
        "print(f'Loss1 numpy : {l1.item()}') #lower \n",
        "print(f'Loss2 numpy : {l2.item()}')\n",
        "\n",
        "_, predictions1 = torch.max(Y_pred_good,1)\n",
        "_, predictions2 = torch.max(Y_pred_bad,1)\n",
        "print(predictions1)\n",
        "print(predictions2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss1 numpy : 0.4170299470424652\n",
            "Loss2 numpy : 1.8167786598205566\n",
            "tensor([0])\n",
            "tensor([1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEkianzgN4U_",
        "colab_type": "code",
        "outputId": "a8962df7-a424-4aff-c0ab-6cfa602dd165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "loss = nn.CrossEntropyLoss() #Softmax at the last layer not required\n",
        "# nn.CrossEntropyLoss() = nn.LogSoftmax + nn.NLLLoss (negative log likelihood loss)\n",
        "\n",
        "# Y actual ( no One Hot required )\n",
        "# Y_pred has raw scores(logits), requires softmax \n",
        "\n",
        "Y = torch.tensor([2])\n",
        "# nsamples x nclasses = 1 x 3\n",
        "Y_pred_bad = torch.tensor([2.0,1.0,0.1]).view(1,3)\n",
        "Y_pred_good = torch.tensor([[0.5,2.0,2]]) #or can be written as this\n",
        "\n",
        "#l1 and l2 is swapped\n",
        "l1 = loss ( Y_pred_good,Y )\n",
        "l2 = loss ( Y_pred_bad, Y )\n",
        "\n",
        "print(f'Loss1 numpy : {l1.item()}') #lower \n",
        "print(f'Loss2 numpy : {l2.item()}') \n",
        "\n",
        "_, predictions1 = torch.max(Y_pred_good,dim=1)\n",
        "_, predictions2 = torch.max(Y_pred_bad,dim=1)\n",
        "print(predictions1)\n",
        "print(predictions2)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss1 numpy : 0.798916220664978\n",
            "Loss2 numpy : 2.3170299530029297\n",
            "tensor([2])\n",
            "tensor([0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYIUBtJSPgfR",
        "colab_type": "text"
      },
      "source": [
        "# Multiple samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTsf7o1qPf-n",
        "colab_type": "code",
        "outputId": "983ca42f-66f2-453c-ee0a-3f9c8a074159",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "loss = nn.CrossEntropyLoss() #Softmax at the last layer not required\n",
        "# nn.CrossEntropyLoss() = nn.LogSoftmax + nn.NLLLoss (negative log likelihood loss)\n",
        "\n",
        "# Y actual ( no One Hot required )\n",
        "# Y_pred has raw scores(logits), requires softmax \n",
        "\n",
        "Y = torch.tensor([2,0,1])\n",
        "\n",
        "# nsamples x nclasses = 3 x 3\n",
        "Y_pred_good = torch.tensor(\n",
        "    [[0.1, 0.2, 3.9], # predict class 2\n",
        "    [1.2, 0.1, 0.3], # predict class 0\n",
        "    [0.3, 2.2, 0.2]]) # predict class 1\n",
        "\n",
        "Y_pred_bad = torch.tensor(\n",
        "    [[0.9, 0.2, 0.1],\n",
        "    [0.1, 0.3, 1.5],\n",
        "    [1.2, 0.2, 0.5]])\n",
        "\n",
        "l1 = loss ( Y_pred_good,Y )\n",
        "l2 = loss ( Y_pred_bad, Y )\n",
        "\n",
        "print(f'Loss1 numpy : {l1.item()}') #lower \n",
        "print(f'Loss2 numpy : {l2.item()}')\n",
        "\n",
        "_, predictions1 = torch.max(Y_pred_good,dim=1)\n",
        "_, predictions2 = torch.max(Y_pred_bad,dim=1)\n",
        "\n",
        "print(predictions1)\n",
        "print(predictions2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss1 numpy : 0.28342217206954956\n",
            "Loss2 numpy : 1.6418448686599731\n",
            "tensor([2, 0, 1])\n",
            "tensor([0, 2, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TeP_KHEYXeO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}